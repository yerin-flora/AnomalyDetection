---
title: "Assignment8_YerinJung"
author: "Yerin Jung"
date: "July 17, 2018"
output:
  prettydoc::html_pretty:
    theme: cayman
    number_sections: yes
    toc: yes
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r packages, echo=FALSE}
setwd("/Users/Jung-yerin/Desktop/SUMMER_2018/AD/Week8")
source("assignment8.R")
```

# Data

  
## Bring the data

  First, I brought 'stock_data.csv' which I created for the first assignment.  
  

```{r data, results='hide'}
setwd("/Users/Jung-yerin/Desktop/SUMMER_2018/AD/Week1/Data/Stocks")
sp500URL <- getURL("https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv")
sp500File <- file.path('data', 'constituents.csv')

if (file.exists(sp500File)) {
  sp500URL <- sp500File
}

# read in the SP500 information
sp500Members <- read_csv( sp500URL )

# read in the one file
stockDF <- read_csv("stock_data.csv", col_types = c("Dddddnnc")) %>%
  arrange(Symbol, Date)

stockDF$Symbol <- toupper(stockDF$Symbol)

sp500DF <- stockDF %>% filter (Symbol %in% sp500Members$Symbol)

gStockDF <- sp500DF %>% 
  group_by(Symbol) %>%
  arrange(Date)

gStockDF <- gStockDF[complete.cases(gStockDF),]
```


## Add new columns 


  Then, I created features to build a model.  
  I additionally created `xxxx_SMA10` features which represent a Simple Moving Average.   
  

```{r features, results='hide'}
# first set of features
df2 <- gStockDF %>% 
  mutate(
    Open_Close_PctChange = (Close-Open)/Open * 100,
    
    Open_Close_Delt = Delt(Open, Close, k = 0) * 100,
    
    Open_Change   = Open   - lag(Open),
    High_Change   = High   - lag(High),
    Low_Change    = Low    - lag(Low),
    Close_Change  = Close  - lag(Close),
    Volume_Change = Volume - lag(Volume),
    
    Daily_Return  = Close / Open,
    
    Open_PctChange   = Open_Change   / lag(Open)   * 100,
    High_PctChange   = High_Change   / lag(High)   * 100,
    Low_PctChange    = Low_Change    / lag(Low)    * 100,
    Close_PctChange  = Close_Change  / lag(Close)  * 100,
    Volume_PctChange = Volume_Change / lag(Volume) * 100,
    
    Open_Mean10   = roll_mean( Open,   10, fill = NA, na.rm = TRUE ),
    High_Mean10   = roll_mean( High,   10, fill = NA, na.rm = TRUE ),
    Low_Mean10    = roll_mean( Low,    10, fill = NA, na.rm = TRUE ),
    Close_Mean10  = roll_mean( Close,  10, fill = NA, na.rm = TRUE ),
    Volume_Mean10 = roll_mean( Volume, 10, fill = NA, na.rm = TRUE ),

    Open_Mean10_R   = roll_meanr( Open,   10, fill = NA, na.rm = TRUE ),
    High_Mean10_R   = roll_meanr( High,   10, fill = NA, na.rm = TRUE ),
    Low_Mean10_R    = roll_meanr( Low,    10, fill = NA, na.rm = TRUE ),
    Close_Mean10_R  = roll_meanr( Close,  10, fill = NA, na.rm = TRUE ),
    Volume_Mean10_R = roll_meanr( Volume, 10, fill = NA, na.rm = TRUE ),
    
    Open_SD30   = roll_sd( Open,   30, fill = NA, na.rm = TRUE ),
    High_SD30   = roll_sd( High,   30, fill = NA, na.rm = TRUE ),
    Low_SD30    = roll_sd( Low,    30, fill = NA, na.rm = TRUE ),
    Close_SD30  = roll_sd( Close,  30, fill = NA, na.rm = TRUE ),
    Volume_SD30 = roll_sd( Volume, 30, fill = NA, na.rm = TRUE ),
    
    Open_VAR30   = roll_var( Open,   30, fill = NA, na.rm = TRUE ),
    High_VAR30   = roll_var( High,   30, fill = NA, na.rm = TRUE ),
    Low_VAR30    = roll_var( Low,    30, fill = NA, na.rm = TRUE ),
    Close_VAR30  = roll_var( Close,  30, fill = NA, na.rm = TRUE ),
    Volume_VAR30 = roll_var( Volume, 30, fill = NA, na.rm = TRUE ),
    
    Open_EMA10   = EMA(Open,   n = 10),
    High_EMA10   = EMA(High,   n = 10),
    Low_EMA10    = EMA(Low,    n = 10),
    Close_EMA10  = EMA(Close,  n = 10),
    Volume_EMA10 = EMA(Volume, n = 10),
    
    Open_SMA10   = SMA(Open,   n = 10),
    High_SMA10   = SMA(High,   n = 10),
    Low_SMA10    = SMA(Low,    n = 10),
    Close_SMA10  = SMA(Close,  n = 10),
    Volume_SMA10 = SMA(Volume, n = 10)

  ) %>%
  arrange(Symbol, Date)
```
  
  
  I joined sp500Members to have only stocks that are S&P 500 members and added DOW and Quarter for the day of the week and quarter respectively.  
  In addition to the DOW, and Quarter, I also added Year and QtrYear, which represent the year and quarter-year respectively. 
  
  
  
```{r features2, results='hide'}
# add SP500 characteristics (sector)
df3 <- df2 %>% left_join(sp500Members, by = "Symbol")

# add weekdays and quarters 
df3$DOW <- weekdays(df3$Date)
df3$Quarter <- quarters(df3$Date)

df3$Year <- as.numeric(format(df3$Date, "%Y"))
df3$QtrYear <- sprintf("%s-%s", df3$Year, df3$Quarter)

```


  
  I selected the year between $2011$ and $2012$ for this assignment.  
  
  
```{r select_year, results='hide'}
qtrs <- df3 %>% 
  ungroup %>% 
  select(Year, QtrYear) %>% 
  unique() %>% 
  filter( between( Year, 2011, 2012) ) %>%
  arrange( QtrYear )

qtrs
```


## Train and Test  

  The following codes represent the way to separate train and test set.  


```{r train_test}
# train/test are 3 and 1 qtrs in duration
trainLen <- 3
testLen <- 1

# rows in the data.frame
trainStart <- 1
trainStop <- nrow(qtrs) - trainLen

# lists we are collecting
trainSet <- list()
testSet <- list()

# identify and print the sets
while( trainStart <= trainStop ) {
  trainQ <- qtrs[seq(trainStart, length.out=trainLen), ]$QtrYear
  testQ <- qtrs[seq(trainStart+trainLen, length.out=testLen), ]$QtrYear
  
  print( "Set:" )
  print( trainQ ) 
  print( testQ )
  print("")
  
  # extract the training ans testing from df3
  trainDF <- df3 %>% filter( QtrYear %in% trainQ )
  testDF <- df3 %>% filter( QtrYear %in% testQ )
  
  key <- sprintf("SET_%d", trainStart)
  
  trainSet[[key]] <- trainDF 
  testSet[[key]] <- testDF 
    
  trainStart <- trainStart + 1
}

```
  
  The following codes represent the way to make several Train-Test sets using a for loop.  
  
  
```{r}
# iterating over the keys
train_dfs <- as.list(rep("", length(trainSet)))
test_dfs <-as.list(rep("", length(trainSet)))

for(i in seq_along(trainSet)) {
  aKey <-names(trainSet)[i]
  train_dfs[[i]] <- trainSet[[aKey]]
  test_dfs[[i]] <- testSet[[aKey]]
} 
```

   
   I am going to quickly check the number of rows in one train/test set.  
   
   
```{r sample_print}
print(sprintf("The number of cases in the first Train set: %s",nrow(train_dfs[[1]])))
print(sprintf("The number of cases in the first Test set: %s",nrow(test_dfs[[1]])))
```

# GLM

  
  I am going to try to use two variables respectively: `Open_Close_PctChange` and `Daily_Return` as target variable.  
  


## Open_Close_PctChange Variable
  
  
  Now, I am going to initiate `h20.glm` model.  
  The following codes exist to implement several train models and bind the results in one single data frame.  
  I selected **outliers** as those which are larger than 5 standard deviations.  
  
  
  
```{r results='hide'}
h2o.init(nthreads=-1, max_mem_size='4G')
h2o.removeAll()
```


```{r results='hide'}
rmse_list <- as.list(rep("", length(train_dfs)))
  
for (i in seq_along(train_dfs)){

  # get ready dataset
  train_test <-ResponseAndData_All("Open_Close_PctChange", train_dfs[[i]], test_dfs[[i]])
  
  # train the model
  results<- TrainAndPlot_3sd(train_test$pred, train_test$resp, 
                                train_test$train.hex, train_test$test.hex, train_test$test)
  
  # Show the result
  rmse_list[[i]] <- sprintf(" Model #%s RMSE: %s", i, results$sqrt_result )
  
  # Outlierss
  std <- sd(results$diff_df$diff)
  
  if (exists("glm_ocpc")){
    temp_ocpc = results$diff_df
    glm_ocpc = rbind(glm_ocpc, temp_ocpc)
    temp_outliers <- temp_ocpc %>%
      filter(abs(diff)>5*std)
    glm_ocpc_outliers=rbind(glm_ocpc_outliers, temp_outliers)
  }
  
  if (!exists("glm_ocpc")){
    glm_ocpc <- results$diff_df
    glm_ocpc_outliers <- glm_ocpc %>%
    filter(abs(diff)>5*std)
    
  }
}
```
  
  
  I printed out each train set's RMSE and the number of outliers (combined).  
  The below table shows the first 5 rows of the detected outliers.  



```{r echo=FALSE}

rmse_list

nrow1 <- nrow(glm_ocpc_outliers)
print( sprintf("The number of outliers (larger than 5 STD) is %s", nrow1) )

kable(head(glm_ocpc_outliers), "html") %>%
  kable_styling((bootstrap_options = c("striped","hover")), full_width = FALSE)
```



```{r}
h2o.shutdown(prompt=TRUE)
```





## Daily_Return Variable

  I conducted the same modeling for the `Daily_Return` variable as well.  
  
  

```{r results='hide'}
h2o.init(nthreads=-1, max_mem_size='4G')
h2o.removeAll()
```

```{r results='hide'}
rmse_list <- as.list(rep("", length(train_dfs)))

for (i in seq_along(train_dfs)){

  # get ready dataset
  train_test <-ResponseAndData_All("Daily_Return", train_dfs[[i]], test_dfs[[i]])
  
  # train the model
  results<- TrainAndPlot_3sd(train_test$pred, train_test$resp, 
                                train_test$train.hex, train_test$test.hex, train_test$test)
  
  # Show the result
   rmse_list[[i]] <- sprintf(" Model #%s RMSE: %s", i, results$sqrt_result )
  
  # Outliers
  std <- sd(results$diff_df$diff)
  
  if (exists("glm_daily")){
   
    temp_daily = results$diff_df
    glm_daily = rbind(glm_daily, temp_daily)
    
    temp_outliers <- temp_daily %>%
      filter(abs(diff)>5*std)
    glm_daily_outliers=rbind(glm_daily_outliers, temp_outliers)
    
  }
  
  if (!exists("glm_daily")){
    
    glm_daily <- results$diff_df
    
    glm_daily_outliers <- glm_daily %>%
    filter(abs(diff)>5*std)
    
  }
}



```

  
  Again, I printed RMSE and the number of outliers.  
  The below table shows the first 5 rows of the detected outliers.  


```{r echo=FALSE}
rmse_list

nrow2 <- nrow(glm_daily_outliers)
print(sprintf("The number of outliers (larger than 5 STD) is %s", nrow2))

kable(head(glm_daily_outliers), "html") %>%
  kable_styling((bootstrap_options = c("striped","hover")), full_width = FALSE)
```


```{r}
h2o.shutdown(prompt=TRUE)
```


# Random Forest

  Although I am going to use the same target variables (`Open_Close_PctChange`, and `Daily_Return`)  
  for the random forest, I selected some of the original variables as predictive variables.  
  This is because non-valuable variables caused worse results for the random forests models.  
  
  

## Open_Close_PctChange Variable
  
  
  

```{r results='hide'}
h2o.init(nthreads=-1, max_mem_size='4G')
h2o.removeAll()
```
  
  
  The below codes represent the way to fit the model and combine the results.  
  
  
```{r results='hide'}
resp <- "Open_Close_PctChange"
pred <- c("Symbol", "DOW", "Sector",
          "Open_Mean10", "Close_Mean10", 
          "Open_Mean10_R", "Close_Mean10_R", 
          "Open_EMA10", "Close_EMA10",
          "Open_SMA10", "Close_SMA10")
  
for (i in seq_along(train_dfs)){

  # get ready dataset
  train_test <-Data_RF(train_dfs[[i]], test_dfs[[i]])

  # train the model
  results<- Train_Plot_RF(pred, resp, train_test$train.hex, train_test$test.hex, train_test$test)
  
  # Outliers
  
  std <- results$std
  
  if (exists("rf_ocpc")){
    
    temp_ocpc = results$diff_df
    rf_ocpc = rbind(rf_ocpc, temp_ocpc)
    
    temp_outliers <- temp_ocpc %>%
      filter(abs(diff)>5*std)
    rf_ocpc_outliers = rbind(rf_ocpc_outliers, temp_outliers)
  }
  
  if (!exists("rf_ocpc")){
    rf_ocpc <- results$diff_df
    rf_ocpc_outliers <- rf_ocpc %>%
    filter(abs(diff)>5*std)
    
  }
}


```
  
  
  As we can see from the below result, around $100$ outliers are detected in this case.  

```{r echo=FALSE}

nrow3 <- nrow(rf_ocpc_outliers)
print(sprintf("The number of outliers (larger than 5 STD) is %s",nrow3))

kable(head(rf_ocpc_outliers), "html") %>%
  kable_styling((bootstrap_options = c("striped","hover")), full_width = FALSE)
```


```{r}
h2o.shutdown(prompt=TRUE)
```



## Daily_Return Variable

  Now, I will apply the same way to predict `Daily_Return`.  
  
  
  
```{r results='hide'}
h2o.init(nthreads=-1, max_mem_size='4G')
h2o.removeAll()
``` 

```{r results='hide'}

resp <- "Daily_Return"
pred <- c("Symbol", "DOW","Sector",
          "Open_Mean10", "Close_Mean10", 
          "Open_Mean10_R", "Close_Mean10_R", 
          "Open_EMA10", "Close_EMA10",
          "Open_SMA10", "Close_SMA10")
  
for (i in seq_along(train_dfs)){

  # get ready dataset
  train_test <-Data_RF(train_dfs[[i]], test_dfs[[i]])

  # train the model
  results<- Train_Plot_RF(pred, resp, train_test$train.hex, train_test$test.hex, train_test$test)
  
  # Outliers
  
  std <- results$std
  
  if (exists("rf_daily")){
    
    temp_daily = results$diff_df
    rf_daily = rbind(rf_daily, temp_daily)
    
    temp_outliers <- temp_daily %>%
      filter(abs(diff)>5*std)
    rf_daily_outliers = rbind(rf_daily_outliers, temp_outliers)
  }
  
  if (!exists("rf_daily")){
    rf_daily <- results$diff_df
    rf_daily_outliers <- rf_daily %>%
    filter(abs(diff)>5*std)
    
  }
}



```
  
  
  Below number of outliers were detected as we can see from the below resutls.
  In addition, the below table shows the first 5 rows of the detected outliers when the predictive variable is `Daily_Return`.    

  
  
```{r echo=FALSE}
print(sprintf("The number of outliers (larger than 5 STD) is %s",nrow(rf_daily_outliers)))

kable(head(rf_daily_outliers), "html") %>%
  kable_styling((bootstrap_options = c("striped","hover")), full_width = FALSE)
```


```{r}
h2o.shutdown(prompt=FALSE)
```



# Business Insights

## Open_Close_PctChange Variable

  
  I plotted the combined results from the two model (GLM and Random Forest) with the real values.  
  As we can see from the below plot, the red points represent the center prediction (which means the average prediction value of glm and random forest), and the blue points & line represent the real value along with the glm prediction x-axis.  
  
  We will see which model brought the distance that we can find from the chart in the next code sections.  
  
  
```{r}
pctChange_df <- data.frame(glm_prediction = glm_ocpc$predictions, 
                           rf_prediction = rf_ocpc$predictions_rf,
                           real_value= glm_ocpc$Open_Close_PctChange)

pctChange_df <- pctChange_df %>%
  mutate(middle_prediction = (glm_prediction + rf_prediction)/2,
         indices=seq(nrow(pctChange_df))) %>%
  select(indices, real_value, glm_prediction, rf_prediction, middle_prediction)
  
pctChange_df %>%
  arrange(glm_prediction)%>%
  ggplot(aes(x=glm_prediction))+
  geom_point(aes(y=middle_prediction), color="red", alpha=0.5)+
  geom_point(aes(y=real_value), color="blue", alpha=0.5)+
  geom_line(aes(y=real_value), color="blue", linetype=2)+
  annotate(geom="text", x=150, y=100, label="Real Value\nline", color="blue")+
  annotate(geom="text", x=0, y=150, label="Predictions'\nCenter Points", color="red")+
  ggtitle("Open_Close_PctChange: Predictions VS. Real Values along")
```
  
  
 If we calculate the euclidean distance between   
 1. GLM Predictions VS. Real Values  
 2. RF Predictions VS. Real Values  
 3. Averaged Predictions VS. Real Values  
 
 As we can find from the below results, glm tends to predict more closely to the real values.  
 

```{r echo=FALSE}
print( sprintf("Error with the GLM model: %s", dist(rbind(pctChange_df$glm_prediction, pctChange_df$real_value))[[1]]) )

print( sprintf ("Error with the Random Forest model: %s", dist(rbind(pctChange_df$rf_prediction, pctChange_df$real_value))[[1]]) )

print( sprintf ("Error with the Combined model: %s", dist(rbind(pctChange_df$middle_prediction, pctChange_df$real_value))[[1]]) )
```


  Finally, I inner joined the two outlier results (`glm_outliers` and `rf_outliers`) to find the commonly detected outliers.  
  Instead of looking at individual values, I decided to look at the `Sector` of the outliers, and look at the percentage of the outliers per each **sector**.  
  
  As we can see from the below table, `Information Technology` sector tend to have more outliers than the other sectors.  
  I assume that IT sector fluctuates the most, while some other sectors with `NA` are relatively stable stocks.  
  
  This would be interesting information for investors who would like to invest stocks based on their *risk sensitivity*. 
  For instance, `Information Technology` stocks might be suitable for investor who are willing to take risk for high return.  
  
  
```{r}
ocpc_outliers_result <- glm_ocpc_outliers %>% inner_join(rf_ocpc_outliers, by=c("sector","symbol","Open_Close_PctChange")) %>%
  select(date.x, sector, symbol, qrt_year.x, Open_Close_PctChange)

ocpc_group <- glm_ocpc %>% 
  group_by(sector) %>%
  summarise(num_count=n())

buz_ocpc <-ocpc_outliers_result %>%
  group_by(sector) %>%
  summarise(num_count=n()) %>%
  right_join(ocpc_group, by="sector") %>%
  mutate(percentage= round( (num_count.x)/num_count.y *100 , 3) )


kable(buz_ocpc, "html") %>%
  kable_styling((bootstrap_options = c("striped","hover")), full_width = FALSE)%>%
  row_spec(7, bold = T, color = "white", background = "skyblue")
```




## Daily_Return Variable
  
  
  I conducted the same way of analysis on the `Daily_Return` variable.  
  I plotted the combined results from the two model (GLM and Random Forest) with the real values.  
  
  
```{r}
DailyReturn_df <- data.frame(glm_prediction = glm_daily$predictions, 
                           rf_prediction = rf_daily$predictions_rf,
                           real_value= glm_daily$Daily_Return)

DailyReturn_df <- DailyReturn_df %>%
  mutate(middle_prediction = (glm_prediction + rf_prediction)/2,
         indices=seq(nrow(DailyReturn_df))) %>%
  select(indices, real_value, glm_prediction, rf_prediction, middle_prediction)
  
DailyReturn_df %>%
  arrange(glm_prediction)%>%
  ggplot(aes(x=glm_prediction))+
  geom_point(aes(y=middle_prediction), color="red", alpha=0.5)+
  geom_point(aes(y=real_value), color="blue", alpha=0.5)+
  geom_line(aes(y=real_value), color="blue", linetype=2)+
  annotate(geom="text", x=3.5, y=5, label="Real Value\nline", color="blue")+
  annotate(geom="text", x=9, y=7.5, label="Predictions'\nCenter Points", color="red")+
  ggtitle("Daily_Return: Predictions VS. Real Values")
```

  
  As we can find from the below results, again, glm tends to predict more closely to the real values.  
  However, in this case, the error turned out to be much lower for both model.  
  
  

```{r echo=FALSE}
print( sprintf("Error with the GLM model: %s", dist(rbind(DailyReturn_df$glm_prediction, DailyReturn_df$real_value))[[1]]) )

print( sprintf ("Error with the Random Forest model: %s", dist(rbind(DailyReturn_df$rf_prediction, DailyReturn_df$real_value))[[1]]) )

print( sprintf ("Error with the Combined model: %s", dist(rbind(DailyReturn_df$middle_prediction, DailyReturn_df$real_value))[[1]]) )
```

  
  
  Again, `Information Technology` showed the highest percentage of outliers.   
  
  This would also be interesting information for investors who would like to invest stocks based on their *risk sensitivity*. 
  
  

```{r}
daily_outliers_result <- glm_daily_outliers %>% inner_join(rf_daily_outliers, by=c("sector","symbol","Daily_Return")) %>%
  select(date.x, sector, symbol, qrt_year.x, Daily_Return)

daily_group <- glm_daily %>% 
  group_by(sector) %>%
  summarise(num_count=n())

buz_daily <-daily_outliers_result %>%
  group_by(sector) %>%
  summarise(num_count=n()) %>%
  right_join(daily_group, by="sector") %>%
  mutate(percentage= round( (num_count.x)/num_count.y *100 , 3) )

kable(buz_daily, "html") %>%
  kable_styling((bootstrap_options = c("striped","hover")), full_width = FALSE)%>%
  row_spec(7, bold = T, color = "white", background = "skyblue")
```




  
  



